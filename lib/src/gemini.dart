import 'package:google_generative_ai/google_generative_ai.dart';
import 'package:http/http.dart' as http;

import 'prompts/prompts.dart';

class GeminiService {
  GeminiService({required String apiKey, required http.Client httpClient})
    : _translatorModel = GenerativeModel(
        // TODO(Amos): ä¹‹åä¹Ÿè®¸å¯ä»¥æ¢æˆå¾®è°ƒåçš„æ¨¡å‹ã€
        //             Gemini 1.5 Proã€Gemini 2.5 Pro æ•ˆæœä¸é”™
        model: translatorModel,
        apiKey: apiKey,
        systemInstruction: Content.system(translatorPrompt),
        generationConfig: GenerationConfig(temperature: 0, topP: 0),
        httpClient: httpClient,
      );

  static const String translatorModel = 'models/gemini-2.0-flash';
  static const String startText = 'å‡†å¤‡å®Œæˆ';
  static const String stopText = 'å…¨éƒ¨è¾“å‡ºå®Œæ¯•';

  final GenerativeModel _translatorModel;

  /// è°ƒç”¨ translator çš„ prompt
  ///
  /// æ•è·å¼‚å¸¸: [GenerativeAIException].
  Future<String> translator(String prompt) {
    return _query(_translatorModel, prompt);
  }

  /// è°ƒç”¨ translator åˆ†å—å¤„ç†çš„ prompt
  ///
  /// æ•è·å¼‚å¸¸: [GenerativeAIException].
  ///
  /// @return
  /// - [outputText] å…¨éƒ¨è¾“å‡ºå†…å®¹
  /// - [totalTokenCount] å½“å‰æ¶ˆè€—çš„æ€» Token
  Future<({String outputText, int totalTokenCount})?> translatorChunk(
    String prompt,
  ) {
    return _queryChunk(_translatorModel, prompt);
  }

  /// å•æ¬¡è¾“å‡º
  Future<String> _query(GenerativeModel model, String prompt) async {
    final response = await model.generateContent([Content.text(prompt)]);
    return (response.text ?? '').trim();
  }

  /// åˆ†å—è¾“å‡º
  ///
  /// @return
  /// - [outputText] å…¨éƒ¨è¾“å‡ºå†…å®¹
  /// - [totalTokenCount] å½“å‰æ¶ˆè€—çš„æ€» Token
  Future<({String outputText, int totalTokenCount})?> _queryChunk(
    GenerativeModel model,
    String prompt,
  ) async {
    var text = '';
    final chat = model.startChat();

    /// å¼€å§‹è¾“å‡º
    final responseStart = await chat.sendMessage(Content.text(prompt));
    if (responseStart.text?.trim() != startText) {
      return null;
    }

    const maxChunk = 50;
    for (var i = 1; i <= maxChunk; i++) {
      print('ğŸ’¬ æ­£åœ¨è¾“å‡º: ç¬¬ $i åˆ†å—');
      final responseNext = await chat.sendMessage(
        Content.text(chunkNextPrompt),
      );
      print('âœ… å®Œæˆè¾“å‡º: ç¬¬ $i åˆ†å—');
      final textNext = responseNext.text ?? '';
      if (textNext.trim() == stopText) {
        break;
      }
      text += textNext;
    }

    final countTokensResponse = await model.countTokens(chat.history);

    return (
      outputText: text.trim(),
      totalTokenCount: countTokensResponse.totalTokens,
    );
  }
}
